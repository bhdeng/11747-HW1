{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "w2v = load_vectors(\"./wiki-news-300d-1M.vec\")\n",
    "for key in w2v.keys():\n",
    "    w2v[key] = np.array(list(w2v[key]))\n",
    "    \n",
    "truncated_vocab = np.load(\"vocab.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2i = defaultdict(lambda: len(t2i))\n",
    "\n",
    "def read_dataset(filename):\n",
    "    embed_data = []\n",
    "    label = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            topic, words = line.lower().strip().split(\" ||| \")\n",
    "            words = words.split(\" \")\n",
    "            embed_line = []\n",
    "            for w in words:\n",
    "                if w in truncated_vocab and w in w2v:\n",
    "                    embed_line.append(np.array(w2v[w]))\n",
    "                else:\n",
    "                    embed_line.append(np.random.random(300))\n",
    "            embed_data.append(np.array(embed_line))\n",
    "            label.append(t2i[topic])\n",
    "    return embed_data, label\n",
    "\n",
    "train_data, train_labels = read_dataset(\"./topicclass/topicclass_train.txt\")\n",
    "test_data, test_labels = read_dataset(\"./topicclass/topicclass_valid_corrected.txt\")\n",
    "true_test_data, _ = read_dataset(\"./topicclass/topicclass_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2v\n",
    "del truncated_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "label2idx = {}\n",
    "for i, label in enumerate(train_labels):\n",
    "    if label in label2idx:\n",
    "        label2idx[label].append(i)\n",
    "    else:\n",
    "        label2idx[label] = [i]\n",
    "        \n",
    "train_idx = []\n",
    "val_idx = []\n",
    "for topic in label2idx:\n",
    "    idx = label2idx[topic]\n",
    "    random.shuffle(idx)\n",
    "    sep = int(0.95 * len(idx))\n",
    "    train_idx.extend(idx[:sep])\n",
    "    val_idx.extend(idx[sep:])\n",
    "    \n",
    "val_data = [train_data[i] for i in val_idx]\n",
    "val_labels = [train_labels[i] for i in val_idx]\n",
    "train_data = [train_data[i] for i in train_idx]\n",
    "train_labels = [train_labels[i] for i in train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max([len(seq) for seq in train_data])\n",
    "for d in range(len(train_data)):\n",
    "    pad_len = max_seq_len - len(train_data[d])\n",
    "    train_data[d] = np.append(train_data[d], np.zeros((pad_len,300)), axis=0)\n",
    "\n",
    "train_data = torch.FloatTensor(train_data)\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "print (\"DONE for training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max([len(seq) for seq in val_data])\n",
    "for d in range(len(val_data)):\n",
    "    pad_len = max_seq_len - len(val_data[d])\n",
    "    val_data[d] = np.append(val_data[d], np.zeros((pad_len,300)), axis=0)\n",
    "\n",
    "val_data = torch.FloatTensor(val_data)\n",
    "val_labels = torch.LongTensor(val_labels)\n",
    "print (\"DONE for validation dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max([len(seq) for seq in test_data])\n",
    "for d in range(len(test_data)):\n",
    "    pad_len = max_seq_len - len(test_data[d])\n",
    "    test_data[d] = np.append(test_data[d], np.zeros((pad_len,300)), axis=0)\n",
    "\n",
    "test_data = torch.FloatTensor(test_data)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "print (\"DONE for testing dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class HW1Dataset(Dataset):\n",
    "    def __init__(self, data, label, has_label=True):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.has_label = has_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if self.has_label:\n",
    "            return self.data[i], self.label[i]\n",
    "        else:\n",
    "            return self.data[i]\n",
    "\n",
    "train_dataset = HW1Dataset(train_data, train_labels)\n",
    "val_dataset = HW1Dataset(val_data, val_labels)\n",
    "test_dataset = HW1Dataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPool(nn.Module):\n",
    "    def __init__(self, embed_size, feat_size, kernel_size):\n",
    "        super(ConvPool,self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=embed_size, out_channels=feat_size, \n",
    "                              kernel_size=kernel_size)\n",
    "#         self.bn = nn.BatchNorm1d(feat_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.conv.weight)\n",
    "#         nn.init.constant_(self.bn.weight, 1)\n",
    "#         nn.init.constant_(self.bn.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_conv = self.conv(x)\n",
    "        out = self.pool(self.relu(x_conv))\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, embed_size, feat_size, kernel_sizes, ntopics, nwords=0):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        #self.embed = nn.Embedding(nwords, embed_size) # batch_size * L * embed_size\n",
    "        self.convs = nn.ModuleList([ConvPool(embed_size, feat_size, i) for i in kernel_sizes])\n",
    "        self.dropout = nn.Dropout(p=0.8)\n",
    "        self.fc1 = nn.Linear(len(kernel_sizes) * feat_size, ntopics)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(feat_size, ntopics)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "#         nn.init.xavier_normal_(self.fc2.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x_embed = self.embed(x).transpose(1,2)\n",
    "        x_embed = x.transpose(1,2)\n",
    "        x_conv = []\n",
    "        for conv in self.convs:\n",
    "            x_conv.append(conv(x_embed))\n",
    "        x_conv = torch.cat(x_conv, dim=1).squeeze()       \n",
    "        out = self.fc1(self.dropout(x_conv))\n",
    "#         out = self.fc2(self.relu(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "feat_size = 128\n",
    "kernel_sizes = [3,4,5]\n",
    "l2_const = 3\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(embed_size, feat_size, kernel_sizes, 16).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_epoch):\n",
    "    model.train()\n",
    "    #scheduler.step()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_acc = 0\n",
    "    num_batch = 0\n",
    "    for data, label in train_loader:\n",
    "        data = data.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        num_batch += 1\n",
    "        \n",
    "        scores = model(data)\n",
    "        #if train_loader.batch_size == 1:\n",
    "        #    scores = scores[np.newaxis,:]\n",
    "        loss = criterion(scores, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # l2 norm constraint\n",
    "        w1 = model.fc1.weight.data\n",
    "        if w1.norm(p=2).item() > l2_const:\n",
    "            w1.div_(w1.norm(p=2) / l2_const)\n",
    "#         w2 = model.fc2.weight.data\n",
    "#         if w2.norm(p=2).item() > l2_const:\n",
    "#             w2.div_(w2.norm(p=2) / l2_const)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = torch.argmax(scores.data, dim=1)\n",
    "        train_acc += torch.sum(prediction.eq(label)).item()\n",
    "\n",
    "    train_loss /= num_batch\n",
    "    train_acc = 1.0 * train_acc / len(train_data)\n",
    "    \n",
    "    print ('[TRAIN] Epoch [%d/%d]    Loss: %.4f    Acc: %.4f' % \n",
    "            (i+1,num_epoch,train_loss,train_acc))\n",
    "    \n",
    "    model.eval()\n",
    "    val_acc = 0\n",
    "    val_loss = 0.0\n",
    "    num_batch = 0\n",
    "    for v_data, v_label in val_loader:\n",
    "        v_data = v_data.to(DEVICE)\n",
    "        v_label = v_label.to(DEVICE)\n",
    "        num_batch += 1\n",
    "        \n",
    "        v_scores = model(v_data)\n",
    "        if v_label.shape == torch.Size([1]):\n",
    "            v_scores = v_scores[np.newaxis,:]\n",
    "        \n",
    "        loss = criterion(v_scores, v_label)        \n",
    "        val_loss += loss.item()\n",
    "        prediction = torch.argmax(v_scores.data, dim=1)\n",
    "        val_acc += torch.sum(prediction.eq(v_label)).item()\n",
    "    \n",
    "    val_loss /= num_batch\n",
    "    val_acc = 1.0 * val_acc / len(val_data)\n",
    "    \n",
    "    print ('[VAL] Epoch [%d/%d]    Loss: %.4f    Acc: %.4f' % \n",
    "            (i+1,num_epoch,val_loss,val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_acc = 0\n",
    "test_loss = 0.0\n",
    "num_batch = 0\n",
    "for t_data, t_label in test_loader:\n",
    "    t_data = t_data.to(DEVICE)\n",
    "    t_label = t_label.to(DEVICE)\n",
    "    num_batch += 1\n",
    "        \n",
    "    t_scores = model(t_data)\n",
    "    if t_label.shape == torch.Size([1]):\n",
    "        t_scores = t_scores[np.newaxis,:]\n",
    "        \n",
    "    loss = criterion(t_scores, t_label)        \n",
    "    test_loss += loss.item()\n",
    "    prediction = torch.argmax(t_scores.data, dim=1)\n",
    "    test_acc += torch.sum(prediction.eq(t_label)).item()\n",
    "    \n",
    "test_loss /= num_batch\n",
    "test_acc = 1.0 * test_acc / len(test_data)\n",
    "    \n",
    "print ('[VAL] Epoch [%d/%d]    Loss: %.4f    Acc: %.4f' % \n",
    "        (i+1,num_epoch,test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./128feat_0.78.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test_dataset = HW1Dataset(true_test_data, None, False)\n",
    "true_test_loader = DataLoader(dataset=true_test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "for tt_data in true_test_loader:\n",
    "    tt_data = tt_data.float().to(DEVICE)\n",
    "    \n",
    "    tt_scores = model(tt_data)[np.newaxis,:]\n",
    "    pred = torch.argmax(tt_scores.data,dim=1)\n",
    "    predictions.append(pred.item())\n",
    "\n",
    "with open(\"prediction.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        for topic, idx in t2i.items():\n",
    "            if idx == pred:\n",
    "                f.write(topic + \"\\n\")\n",
    "                break\n",
    "print (\"finish prediction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
